# -*- coding: utf-8 -*-
"""PrincessDonkor._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YwvgAfnHpE3hLATDIeg1hWTeHiLhksC3
"""

import pandas as pd
import numpy as np
import os
import sklearn
from sklearn import tree, metrics
from sklearn.model_selection import train_test_split
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import random as rnd
import xgboost as xgb

drive.mount('/content/drive')

maleplayers_df= pd.read_csv('/content/drive/MyDrive/male_players (legacy).csv')

players22_df = pd.read_csv('/content/drive/MyDrive/players_22-1.csv')

maleplayers_df.info()

maleplayers_df.head()

players22_df.info()

players22_df.head()

"""**Data preparation & feature extraction process**"""

missing_values_male = maleplayers_df.isnull().sum()
missing_values_players = players22_df.isnull().sum()

missing_values_male

maleplayers_df.fillna(maleplayers_df.mean,inplace=True)
players22_df.fillna(players22_df.mean,inplace=True)
maleplayers_df = maleplayers_df[maleplayers_df.columns.drop(list(maleplayers_df.filter(regex='url')))]
players22_df = players22_df[players22_df.columns.drop(list(players22_df.filter(regex='url')))]

maleplayers_df

maleplayers_df.columns

dropped_columns = ['long_name','dob','club_team_id']

maleplayers_df = maleplayers_df.drop(columns = dropped_columns)

players22_df = players22_df.drop(columns=dropped_columns)

maleplayers_df

"""**Feature subsets that show maximum correlation with the dependent variable**"""

maleplayers_df['total_stats'] = maleplayers_df[['weak_foot','pace','shooting','passing','dribbling','defending','physic']].sum(axis=1, numeric_only=True)
chosen_columns = ['weak_foot', 'pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic']

maleplayers_df.loc[:,'total_stats'] = maleplayers_df[chosen_columns].sum(axis=1, numeric_only=True)
players22_df.loc[:,'total_stats'] = players22_df[chosen_columns].sum(axis=1, numeric_only=True)

correlation = maleplayers_df.corr(numeric_only=True)

total_correlation= correlation['overall'].abs().sort_values(ascending=False)

#features with the maximum correlation with the dependent variable
total_correlation

top=10
features = total_correlation[1:top+1].index

features

"""**Training a suitable machine learning model with cross-validation that can predict a player's rating**"""

StandardScaler = StandardScaler()

ML_model= RandomForestRegressor(n_estimators=500,random_state=42)

X = maleplayers_df[features]

y = maleplayers_df['overall']

X

X = StandardScaler.fit_transform(X.copy())

X = pd.DataFrame(X,columns = features)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=30,max_depth=30,min_samples_leaf=2,min_samples_split=5,random_state=42)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

feature_importances = ML_model.feature_importances_

feature_importances

feature_subset = maleplayers_df[features]

feature_subset.head()

#initialize and fit xgbregressor model to the training data.
modelxgb = xgb.XGBRegressor()
modelxgb.fit(X_train,y_train)

cross_val_score(modelxgb, X_test, y_test, cv=5)

y_xgb = modelxgb.predict(X_test)

mean_abs_err = mean_absolute_error(y_test,y_xgb)

mean_sq_err = mean_squared_error(y_test,y_xgb)

r2 = r2_score(y_test,y_xgb)

print(f"Mean Absolute Error: {mean_abs_err}")
print(f"Mean Squared Error: {mean_sq_err}")
print(f"R-squared: {r2}")

parameter_grid= {
    'n_estimators': [10, 20, 30],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

Grid_Search = GridSearchCV(ML_model,parameter_grid,cv=10,scoring='neg_mean_squared_error')

Grid_Search.fit(X_train,y_train)

model_pred = np.round(y_pred).astype(int)
model_pred_xgb = np.round(y_xgb).astype(int)

results = pd.DataFrame({
    'Actual': y_test,
    'Predicted': model_pred,
    'Predicted_XGB': model_pred_xgb
})

print(results.head(10))

#choosing the best hyperparameter
best_parameters = Grid_Search.best_params_

best_parameters

from sklearn.linear_model import LinearRegression

# Make sure the columns are aligned
X_train = players22_df[chosen_columns].dropna()
y_train = players22_df.loc[X_train.index, 'overall']

# Initialize the model
lr_model = LinearRegression()

# Fit the model on the training data
lr_model.fit(X_train, y_train)

X_val = players22_df[chosen_columns].dropna()
y_val = players22_df.loc[X_val.index, 'overall']

# Make predictions on the validation set
linear_y_val_pred = lr_model.predict(X_val)

# Calculate RMSE on the validation set
lr_val_rmse = np.sqrt(mean_squared_error(y_val, linear_y_val_pred))
print(f"Linear Regression Validation RMSE: {lr_val_rmse}")

from sklearn.linear_model import Ridge

# Define the parameter grid for alpha
param_grid = {'alpha': [0.1, 1, 10, 100]}

# Initialize the Ridge model
ridge = Ridge()

# Initialize GridSearchCV
ridge_grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid,
                                 scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=2)

# Fit GridSearchCV to the data
ridge_grid_search.fit(X_train, y_train)

# View the best parameters and the best score
print(f"Best parameters for Ridge: {ridge_grid_search.best_params_}")
print(f"Best RMSE for Ridge: {np.sqrt(-ridge_grid_search.best_score_)}")

# Train the final Ridge model using the best parameters
best_ridge_model = ridge_grid_search.best_estimator_

# Fit the model on the entire training data
best_ridge_model.fit(X_train, y_train)

# Make predictions on the validation set
y_val_pred_ridge = best_ridge_model.predict(X_val)

# Calculate RMSE on the validation set
ridge_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_ridge))
print(f"Ridge Regression Validation RMSE: {ridge_val_rmse}")

"""### **Testing Using Players from Fifa 2022**"""

players22_df.head()

#convert chosen columns to numeric
players22_df[chosen_columns] = players22_df[chosen_columns].apply(pd.to_numeric, errors='coerce')

players22_df.loc[:,'total_stats'] = players22_df[chosen_columns].sum(axis=1)

non_numeric_cols = players22_df.select_dtypes(exclude=[np.number]).columns
print("Non-numeric columns:", non_numeric_cols)

correlation = players22_df.corr()

total_correlation = correlation['overall'].abs().sort_values(ascending=False)
total_correlation

top=10
important_features = total_correlation[1:top+1].index
important_features

X = players22_df[important_features]
y = players22_df['overall']

StandardScaler=StandardScaler

X = StandardScaler.fit_transform(X.copy())
X = pd.DataFrame(X,columns = important_features)

"""**Testing The Performance of Players with Random Regressor**"""

rf_model = RandomForestRegressor(n_estimators=30,max_depth=30,min_samples_leaf=2)

rf_model.fit(X_train,y_train)

X_test = players22_df[chosen_columns].dropna()
y_test = players22_df.loc[X_test.index, 'overall']

#to re-align y_test to ensure it matches the rows of X_test
y_test = y_test.loc[X_test.index]

y_predict = rf_model.predict(X_test)
y_predict

print(mean_absolute_error(y_test, y_pred))

"""**Making Predictions with The XGB Regressor Model**"""

#initialize and fit xgb model
modelxgb = xgb.XGBRegressor()
modelxgb.fit(X_train, y_train)

#to make predictions on the test set
y_predicted = modelxgb.predict(X_test)

print(f"Mean Absolute Error for XGBRegressor: {mean_absolute_error(y_test, y_pred)}")

"""**Using Pickle to Save The Model**"""

import bz2
import pickle
import joblib
import xgboost as xgb
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

#save scaler
scaler_filename = '/content/drive/My Drive/Intro to AI/scaler.pkl'
joblib.dump(scaler, scaler_filename)

#train and save model
modelxgb = xgb.XGBRegressor()
modelxgb.fit(X_train_scaled, y_train)
model_filename = '/content/drive/My Drive/Intro to AI/modelxgb.pkl'
joblib.dump(modelxgb, model_filename)

